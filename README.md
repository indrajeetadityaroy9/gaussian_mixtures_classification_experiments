# Classification Experiments with Gaussian Mixtures

An end-to-end workflow for experimenting with 3 distinct classification strategies applied to a 4-dimensional random vector drawn from a mixture of 2 Gaussian distributions highlighting how varying modeling assumptions and parameter estimation methods influence decision boundaries, error rates, and overall classification performance.

A binary classification task where the data X is generated by one of two classes. Each class is modeled as a multivariate Gaussian distribution with known mean and covariance matrix. The overall data distribution is:

$$
p(x) = p(x|L=0)P(L=0) + p(x|L=1)P(L=1), \text{   with class priors   } P(L=0) = 0.35 \text{   and   } P(L=1)=0.65
$$

## Classification Scenarios

1. **Part A: Minimum Expected Risk (Bayes) Classification**  
   - **Assumptions:** Full knowledge of class conditional distributions (means & covariances) and class priors.  
   - **Method:** Apply the Bayes decision rule under 0-1 loss, using the true parameters to achieve the optimal decision boundary.
     
2. **Part B: Naive Bayesian Classification**  
   - **Assumptions:** Knowledge of class means and priors, but incorrect assumption of feature independence (diagonal covariances).  
   - **Method:** Approximate class conditional pdfs ignoring correlations. Compare resulting performance to Part A.

3. **Part C: Fisher Linear Discriminant Analysis (LDA)**  
   - **Assumptions:** Parameters unknown and estimated from data; assumes equal covariance matrices for both classes.  
   - **Method:** Compute a linear discriminant that maximally separates classes, project data onto this 1D space, and choose a threshold.  

## Overview

1. **Data Generation:**  
   - Generates 10,000 samples from the specified Gaussian mixture distribution with given priors and parameters.

2. **Classification Strategies:**
   - **Bayes Classifier:** Uses the true model parameters for optimal classification.
   - **Naive Bayes Classifier:** Applies a simplified independence assumption, resulting in a less accurate model.
   - **LDA Classifier:** Estimates parameters from data and applies LDA for classification on a projected axis.

3. **Evaluation:**
   - Produces ROC curves for each classifier, illustrating the trade-off between TPR and FPR as the decision threshold changes.
   - Estimates the minimum probability of error for each classifier, comparing how assumptions and estimation quality affect performance.

## Results and Interpretation

- **Bayes Classifier (Part A):**  
  Achieves the theoretically optimal performance, serving as a benchmark.

- **Naive Bayes Classifier (Part B):**  
  Demonstrates worse performance due to ignoring feature correlations, increasing the minimum achievable error.

- **LDA Classifier (Part C):**  
  Falls between the Bayes and Naive Bayes methods. While parameter estimation and linear assumptions impose restrictions, it generally performs better than the naive approach but not as well as the fully informed Bayes classifier.
